{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tft import MyGatedResidualNetwork, MyResampleNorm\n",
    "\n",
    "class MyVariableSelectionNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_sizes: Dict[str, int],\n",
    "        hidden_size: int,\n",
    "        input_embedding_flags: Dict[str, bool] = {},\n",
    "        dropout: float = 0.1,\n",
    "        context_size: int = None,\n",
    "        single_variable_grns: Dict[str, MyGatedResidualNetwork] = {},\n",
    "        prescalers: Dict[str, nn.Linear] = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate weights for ``num_inputs`` variables  which are each of size ``input_size``\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_sizes = input_sizes\n",
    "        self.input_embedding_flags = input_embedding_flags\n",
    "        self.dropout = dropout\n",
    "        self.context_size = context_size\n",
    "\n",
    "        if self.num_inputs > 1:\n",
    "            if self.context_size is not None:\n",
    "                self.flattened_grn = MyGatedResidualNetwork(\n",
    "                    self.input_size_total,\n",
    "                    min(self.hidden_size, self.num_inputs),\n",
    "                    self.num_inputs,\n",
    "                    self.dropout,\n",
    "                    self.context_size,\n",
    "                    residual=False,\n",
    "                )\n",
    "            else:\n",
    "                self.flattened_grn = MyGatedResidualNetwork(\n",
    "                    self.input_size_total,\n",
    "                    min(self.hidden_size, self.num_inputs),\n",
    "                    self.num_inputs,\n",
    "                    self.dropout,\n",
    "                    residual=False,\n",
    "                )\n",
    "\n",
    "        self.single_variable_grns = nn.ModuleDict()\n",
    "        self.prescalers = nn.ModuleDict()\n",
    "        for name, input_size in self.input_sizes.items():\n",
    "            if name in single_variable_grns:\n",
    "                self.single_variable_grns[name] = single_variable_grns[name]\n",
    "            elif self.input_embedding_flags.get(name, False):\n",
    "                self.single_variable_grns[name] = MyResampleNorm(\n",
    "                    input_size, self.hidden_size\n",
    "                )\n",
    "            else:\n",
    "                self.single_variable_grns[name] = MyGatedResidualNetwork(\n",
    "                    input_size,\n",
    "                    min(input_size, self.hidden_size),\n",
    "                    output_size=self.hidden_size,\n",
    "                    dropout=self.dropout,\n",
    "                )\n",
    "            if name in prescalers:  # reals need to be first scaled up\n",
    "                self.prescalers[name] = prescalers[name]\n",
    "            elif not self.input_embedding_flags.get(name, False):\n",
    "                self.prescalers[name] = nn.Linear(1, input_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    @property\n",
    "    def input_size_total(self):\n",
    "        return sum(\n",
    "            size if name in self.input_embedding_flags else size\n",
    "            for name, size in self.input_sizes.items()\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_inputs(self):\n",
    "        return len(self.input_sizes)\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor], context: torch.Tensor = None):\n",
    "        if self.num_inputs > 1:\n",
    "            # transform single variables\n",
    "            var_outputs = []\n",
    "            weight_inputs = []\n",
    "            for name in self.input_sizes.keys():\n",
    "                # select embedding belonging to a single input\n",
    "                variable_embedding = x[name]\n",
    "                if name in self.prescalers:\n",
    "                    variable_embedding = self.prescalers[name](variable_embedding)\n",
    "                weight_inputs.append(variable_embedding)\n",
    "                var_outputs.append(self.single_variable_grns[name](variable_embedding))\n",
    "            var_outputs = torch.stack(var_outputs, dim=-1)\n",
    "\n",
    "            # calculate variable weights\n",
    "            ## get all of the embeddings from all of the variables and just combine them, very simple\n",
    "            flat_embedding = torch.cat(weight_inputs, dim=-1)\n",
    "            #\n",
    "            sparse_weights = self.flattened_grn(flat_embedding, context)\n",
    "            sparse_weights = self.softmax(sparse_weights).unsqueeze(-2)\n",
    "\n",
    "            outputs = var_outputs * sparse_weights\n",
    "            outputs = outputs.sum(dim=-1)\n",
    "\n",
    "        else:  # for one input, do not perform variable selection, just encoding\n",
    "            pass\n",
    "            \n",
    "\n",
    "        # the outputs are a weighted sum of the importance of each variable for the current time step?\n",
    "        return outputs, sparse_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs shape: torch.Size([8, 64])\n",
      "Weights shape: torch.Size([8, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "x = {\n",
    "    \"fueltype\": torch.tensor(\n",
    "        [\n",
    "            [1.3122, 2.0916, 0.4749, 2.5620, -2.2733],\n",
    "            [0.3703, -1.0351, -0.2936, 1.7159, 0.6043],\n",
    "            [0.6815, -0.5216, -0.5855, -1.4212, 0.9495],\n",
    "            [-0.4051, -1.1760, 0.8423, -0.3982, 0.0264],\n",
    "            [1.6051, 1.6294, -1.8574, -0.9640, -0.3509],\n",
    "            [0.5635, -1.2075, -0.0809, -1.2652, -0.8209],\n",
    "            [-2.4415, -0.5139, -0.5364, -0.1024, 0.3291],\n",
    "            [0.5885, 0.1993, -1.9647, -0.0054, -1.3004],\n",
    "        ],\n",
    "        device=\"cpu\",\n",
    "    ),\n",
    "    \"encoder_length\": torch.tensor(\n",
    "        [[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]], device=\"cpu\"\n",
    "    ),\n",
    "    \"value_center\": torch.tensor(\n",
    "        [\n",
    "            [0.5803],\n",
    "            [1.6990],\n",
    "            [1.4191],\n",
    "            [-0.8215],\n",
    "            [-0.7555],\n",
    "            [-0.8013],\n",
    "            [-0.7047],\n",
    "            [-0.6153],\n",
    "        ],\n",
    "        device=\"cpu\",\n",
    "    ),\n",
    "    \"value_scale\": torch.tensor(\n",
    "        [\n",
    "            [1.3127],\n",
    "            [2.0438],\n",
    "            [-0.2789],\n",
    "            [-0.7635],\n",
    "            [-0.7440],\n",
    "            [-0.6782],\n",
    "            [-0.5635],\n",
    "            [-0.3283],\n",
    "        ],\n",
    "        device=\"cpu\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")  # or torch.device('cuda') if using GPU and it's available\n",
    "\n",
    "x = {name: tensor.to(device) for name, tensor in x.items()}\n",
    "\n",
    "# Create an instance of MyVariableSelectionNetwork\n",
    "vsn = MyVariableSelectionNetwork(\n",
    "    input_sizes={'fueltype': 5, 'encoder_length': 16, 'value_center': 16, 'value_scale': 16},\n",
    "    hidden_size=64,\n",
    "    input_embedding_flags={'fueltype':True},\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "vsn.to(device)\n",
    "\n",
    "\n",
    "outputs, weights = vsn(x)\n",
    "\n",
    "# Print the shapes of the outputs and weights\n",
    "print(\"Outputs shape:\", outputs.shape)\n",
    "print(\"Weights shape:\", weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1079, -0.1849, -0.0550,  0.1918,  0.1365,  0.3658,  0.4432,  0.3992,\n",
       "         0.7487,  0.7473,  0.7420,  0.6947,  0.6873,  0.4913,  0.4800,  0.5437,\n",
       "         0.8107,  0.4517,  0.7231,  0.4370,  0.5048,  0.1436,  0.2879,  0.1371,\n",
       "         0.0751,  0.1137, -0.1464, -0.2281, -0.3206, -0.3082, -0.3979, -0.3520,\n",
       "        -0.1593, -0.0053,  0.2898, -0.0181, -0.0355, -0.2747, -0.2707,  0.0326,\n",
       "         0.1103,  0.0876,  0.1589,  0.1618,  0.1073,  0.0519,  0.2245,  0.4107,\n",
       "         0.3577,  0.2566, -0.0041, -0.0374, -0.0417, -0.1023, -0.1377, -0.1823,\n",
       "        -0.3507, -0.5448, -0.7976, -0.8944, -1.1224, -1.5733, -1.9310, -2.0214],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6045, 0.0711, 0.2730, 0.0514]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['max_pred_len'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_data_path': '/home/luke/projects/jupyterlab/Notebooks/tg/tft/data/power_consumption_by_fuel_type.csv',\n",
       " 'target': 'value',\n",
       " 'group_ids': ['fueltype'],\n",
       " 'static_categoricals': ['fueltype'],\n",
       " 'static_reals': ['encoder_length', 'value_center', 'value_scale'],\n",
       " 'time_windows': [2, 4, 12, 24, 48],\n",
       " 'large_time_windows': [168, 730, 8760],\n",
       " 'max_pred_len': 100,\n",
       " 'max_encoder_len': 1344,\n",
       " 'min_encoder_len': 336,\n",
       " 'training_cutoff_quantile': 0.9,\n",
       " 'batch_size': 64,\n",
       " 'val_batch_size': 8,\n",
       " 'trainer_params': {'accelerator': 'gpu',\n",
       "  'max_epochs': 100,\n",
       "  'enable_model_summary': True,\n",
       "  'gradient_clip_val': 0.014,\n",
       "  'limit_train_batches': 100,\n",
       "  'callbacks': [<lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint at 0x7cf40352f160>,\n",
       "   <lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor at 0x7cf1ab6ab4c0>,\n",
       "   <lightning.pytorch.callbacks.early_stopping.EarlyStopping at 0x7cf1bfe7cd30>],\n",
       "  'logger': <lightning.pytorch.loggers.tensorboard.TensorBoardLogger at 0x7cf403527cd0>},\n",
       " 'tft_params': {'hidden_size': 32,\n",
       "  'lstm_layers': 2,\n",
       "  'dropout': 0.5,\n",
       "  'output_size': 7,\n",
       "  'loss': QuantileLoss(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]),\n",
       "  'attention_head_size': 4,\n",
       "  'learning_rate': 0.003,\n",
       "  'hidden_continuous_size': 16,\n",
       "  'hidden_continuous_sizes': {},\n",
       "  'log_interval': 10,\n",
       "  'log_val_interval': 10,\n",
       "  'log_gradient_flow': False,\n",
       "  'optimizer': 'Ranger',\n",
       "  'reduce_on_plateau_patience': 4,\n",
       "  'time_varying_categoricals_encoder': [],\n",
       "  'time_varying_categoricals_decoder': [],\n",
       "  'categorical_groups': {},\n",
       "  'embedding_sizes': {'fueltype': (8, 5)},\n",
       "  'embedding_paddings': [],\n",
       "  'embedding_labels': {'fueltype': {'COL': 0,\n",
       "    'NG': 1,\n",
       "    'NUC': 2,\n",
       "    'OIL': 3,\n",
       "    'OTH': 4,\n",
       "    'SUN': 5,\n",
       "    'WAT': 6,\n",
       "    'WND': 7}},\n",
       "  'monotone_constaints': {},\n",
       "  'share_single_variable_networks': False,\n",
       "  'causal_attention': True,\n",
       "  'logging_metrics': ModuleList(\n",
       "    (0): SMAPE()\n",
       "    (1): MAE()\n",
       "    (2): RMSE()\n",
       "    (3): MAPE()\n",
       "  ),\n",
       "  'output_transformer': GroupNormalizer(\n",
       "  \tmethod='standard',\n",
       "  \tgroups=['fueltype'],\n",
       "  \tcenter=True,\n",
       "  \tscale_by_group=False,\n",
       "  \ttransformation='softplus',\n",
       "  \tmethod_kwargs={}\n",
       "  )},\n",
       " 'time_varying_known_real_additions': ['year']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_file(dir_name):\n",
    "    path = f\"checkpoints/{dir_name}\"\n",
    "    try:\n",
    "        files = next(os.walk(path))[2]  # Get list of files in the directory\n",
    "        if files:  # Check if the list is not empty\n",
    "            return os.path.join(path, files[0])  # Return the first file with full path\n",
    "        else:\n",
    "            return \"No files found in the directory.\"\n",
    "    except StopIteration:\n",
    "        return \"Directory does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/1/test.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_table_24 = [24 * i for i in range(1, 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 96, 168, 240, 312, 384, 456, 528, 600, 672, 744, 816, 888]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_table_24[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_pipeline import run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "training model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2024-03-14 09:28:47.847281: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-14 09:28:47.848182: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-14 09:28:47.864020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 09:28:48.196526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                              | Params\n",
      "------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                      | 0     \n",
      "1  | logging_metrics                    | ModuleList                        | 0     \n",
      "2  | input_embeddings                   | MyMultiEmbedding                  | 40    \n",
      "3  | prescalers                         | ModuleDict                        | 1.2 K \n",
      "4  | static_variable_selection          | MyVariableSelectionNetwork        | 5.9 K \n",
      "5  | encoder_variable_selection         | MyVariableSelectionNetwork        | 81.5 K\n",
      "6  | decoder_variable_selection         | MyVariableSelectionNetwork        | 34.7 K\n",
      "7  | static_context_variable_selection  | MyGatedResidualNetwork            | 4.3 K \n",
      "8  | static_context_initial_hidden_lstm | MyGatedResidualNetwork            | 4.3 K \n",
      "9  | static_context_initial_cell_lstm   | MyGatedResidualNetwork            | 4.3 K \n",
      "10 | static_context_enrichment          | MyGatedResidualNetwork            | 4.3 K \n",
      "11 | lstm_encoder                       | LSTM                              | 16.9 K\n",
      "12 | lstm_decoder                       | LSTM                              | 16.9 K\n",
      "13 | post_lstm_gate_encoder             | MyGatedLinearUnit                 | 2.1 K \n",
      "14 | post_lstm_add_norm_encoder         | MyAddNorm                         | 64    \n",
      "15 | static_enrichment                  | MyGatedResidualNetwork            | 5.3 K \n",
      "16 | multihead_attn                     | MyInterpretableMultiHeadAttention | 2.6 K \n",
      "17 | post_attn_gate_norm                | MyGatedAddNorm                    | 2.2 K \n",
      "18 | pos_wise_ff                        | MyGatedResidualNetwork            | 4.3 K \n",
      "19 | pre_output_gate_norm               | MyGatedAddNorm                    | 2.2 K \n",
      "20 | output_layer                       | Linear                            | 231   \n",
      "------------------------------------------------------------------------------------------\n",
      "191 K     Trainable params\n",
      "0         Non-trainable params\n",
      "191 K     Total params\n",
      "0.766     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741af48d30084482bf0ef9f3578e8e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d324873b804f4a97a1dba37625fe7ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7218fb08752545fba622d011c40e5c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training model: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "training model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                              | Params\n",
      "------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                      | 0     \n",
      "1  | logging_metrics                    | ModuleList                        | 0     \n",
      "2  | input_embeddings                   | MyMultiEmbedding                  | 40    \n",
      "3  | prescalers                         | ModuleDict                        | 1.2 K \n",
      "4  | static_variable_selection          | MyVariableSelectionNetwork        | 5.9 K \n",
      "5  | encoder_variable_selection         | MyVariableSelectionNetwork        | 81.5 K\n",
      "6  | decoder_variable_selection         | MyVariableSelectionNetwork        | 34.7 K\n",
      "7  | static_context_variable_selection  | MyGatedResidualNetwork            | 4.3 K \n",
      "8  | static_context_initial_hidden_lstm | MyGatedResidualNetwork            | 4.3 K \n",
      "9  | static_context_initial_cell_lstm   | MyGatedResidualNetwork            | 4.3 K \n",
      "10 | static_context_enrichment          | MyGatedResidualNetwork            | 4.3 K \n",
      "11 | lstm_encoder                       | LSTM                              | 16.9 K\n",
      "12 | lstm_decoder                       | LSTM                              | 16.9 K\n",
      "13 | post_lstm_gate_encoder             | MyGatedLinearUnit                 | 2.1 K \n",
      "14 | post_lstm_add_norm_encoder         | MyAddNorm                         | 64    \n",
      "15 | static_enrichment                  | MyGatedResidualNetwork            | 5.3 K \n",
      "16 | multihead_attn                     | MyInterpretableMultiHeadAttention | 2.6 K \n",
      "17 | post_attn_gate_norm                | MyGatedAddNorm                    | 2.2 K \n",
      "18 | pos_wise_ff                        | MyGatedResidualNetwork            | 4.3 K \n",
      "19 | pre_output_gate_norm               | MyGatedAddNorm                    | 2.2 K \n",
      "20 | output_layer                       | Linear                            | 231   \n",
      "------------------------------------------------------------------------------------------\n",
      "191 K     Trainable params\n",
      "0         Non-trainable params\n",
      "191 K     Total params\n",
      "0.766     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9100a53faf954c4eb47ff52b20bc735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/luke/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5b0c351ab3427fb3ec9c0a75c8290c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
